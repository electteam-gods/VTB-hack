{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12976483,"sourceType":"datasetVersion","datasetId":8213237}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install python-docx\n!pip install striprtf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2T51qN3Ormkc","outputId":"e744f6ab-4a2f-4a8f-9490-163831ccc7f5","trusted":true,"execution":{"iopub.status.busy":"2025-09-06T08:44:22.812804Z","iopub.execute_input":"2025-09-06T08:44:22.813026Z","iopub.status.idle":"2025-09-06T08:44:31.831093Z","shell.execute_reply.started":"2025-09-06T08:44:22.812995Z","shell.execute_reply":"2025-09-06T08:44:31.830366Z"}},"outputs":[{"name":"stdout","text":"Collecting python-docx\n  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\nRequirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.0)\nDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: python-docx\nSuccessfully installed python-docx-1.2.0\nCollecting striprtf\n  Downloading striprtf-0.0.29-py3-none-any.whl.metadata (2.3 kB)\nDownloading striprtf-0.0.29-py3-none-any.whl (7.9 kB)\nInstalling collected packages: striprtf\nSuccessfully installed striprtf-0.0.29\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import re\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom dataclasses import dataclass\nimport docx\nfrom google.colab import files\nimport io\nfrom striprtf.striprtf import rtf_to_text\n\n@dataclass\nclass Vacancy:\n    title: Optional[str] = None\n    city: Optional[str] = None\n    employment_type: Optional[str] = None\n    work_schedule: Optional[str] = None\n    responsibilities: List[str] = None\n    requirements: List[str] = None\n    advantages: List[str] = None  # –ù–æ–≤–æ–µ –ø–æ–ª–µ –¥–ª—è \"–ë—É–¥–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ–º\"\n    education_level: Optional[str] = None\n    experience_required: Optional[str] = None\n\n    def __post_init__(self):\n        if self.responsibilities is None:\n            self.responsibilities = []\n        if self.requirements is None:\n            self.requirements = []\n        if self.advantages is None:\n            self.advantages = []\n\nclass AdvancedVacancyParser:\n    \"\"\"–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –≤–∞–∫–∞–Ω—Å–∏–π —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å–ª–æ–∂–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä\"\"\"\n\n    def parse_file(self, file_content: bytes, filename: str) -> Vacancy:\n        \"\"\"–ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞ –≤–∞–∫–∞–Ω—Å–∏–∏\"\"\"\n        try:\n            if filename.lower().endswith('.docx'):\n                return self._parse_docx_vacancy(file_content)\n            elif filename.lower().endswith('.rtf'):\n                return self._parse_rtf_vacancy(file_content)\n            else:\n                raise ValueError(f\"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {filename}\")\n\n        except Exception as e:\n            print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–∏: {e}\")\n            return Vacancy()\n\n    def _parse_docx_vacancy(self, file_content: bytes) -> Vacancy:\n        \"\"\"–ü–∞—Ä—Å–∏–Ω–≥ DOCX –≤–∞–∫–∞–Ω—Å–∏–∏\"\"\"\n        doc = docx.Document(io.BytesIO(file_content))\n        vacancy_data = {}\n\n        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã\n        for table in doc.tables:\n            for row in table.rows:\n                if len(row.cells) >= 2:\n                    field_name = row.cells[0].text.strip()\n                    field_value = row.cells[1].text.strip()\n                    if field_name and field_value:\n                        vacancy_data[field_name] = field_value\n\n        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å–ª–æ–∂–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞\n        full_text = \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n\n        return self._parse_vacancy_data(vacancy_data, full_text)\n\n    def _parse_rtf_vacancy(self, file_content: bytes) -> Vacancy:\n        \"\"\"–ü–∞—Ä—Å–∏–Ω–≥ RTF –≤–∞–∫–∞–Ω—Å–∏–∏\"\"\"\n        rtf_text = file_content.decode('utf-8', errors='ignore')\n        text = rtf_to_text(rtf_text)\n        vacancy_data = {}\n\n        # –ü–∞—Ä—Å–∏–º RTF —Ç–µ–∫—Å—Ç\n        lines = text.split('\\n')\n        for line in lines:\n            line = line.strip()\n            if '|' in line:  # –¢–∞–±–ª–∏—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç\n                parts = [p.strip() for p in line.split('|')]\n                if len(parts) >= 2:\n                    vacancy_data[parts[0]] = '|'.join(parts[1:])\n            elif ':' in line and len(line.split(':')) == 2:  # –ö–ª—é—á: –∑–Ω–∞—á–µ–Ω–∏–µ\n                key, value = [p.strip() for p in line.split(':', 1)]\n                vacancy_data[key] = value\n\n        return self._parse_vacancy_data(vacancy_data, text)\n\n    def _parse_vacancy_data(self, vacancy_data: Dict[str, str], full_text: str) -> Vacancy:\n        \"\"\"–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π\"\"\"\n        vacancy = Vacancy()\n\n        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è\n        vacancy.title = self._get_field_value(vacancy_data, '–ù–∞–∑–≤–∞–Ω–∏–µ')\n        vacancy.city = self._get_field_value(vacancy_data, '–ì–æ—Ä–æ–¥')\n        vacancy.employment_type = self._get_field_value(vacancy_data, '–¢–∏–ø –∑–∞–Ω—è—Ç–æ—Å—Ç–∏')\n        vacancy.work_schedule = self._get_field_value(vacancy_data, '–¢–µ–∫—Å—Ç –≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã')\n        vacancy.education_level = self._get_field_value(vacancy_data, '–£—Ä–æ–≤–µ–Ω—å –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è')\n        vacancy.experience_required = self._get_field_value(vacancy_data, '–¢—Ä–µ–±—É–µ–º—ã–π –æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã')\n\n        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º\n        responsibilities_text = self._get_field_value(vacancy_data, '–û–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ (–¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏)')\n        requirements_text = self._get_field_value(vacancy_data, '–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è (–¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏)')\n\n        # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞\n        if requirements_text:\n            requirements, advantages = self._parse_requirements_with_advantages(requirements_text)\n            vacancy.requirements = requirements\n            vacancy.advantages = advantages\n        else:\n            # –ò—â–µ–º –≤ –ø–æ–ª–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\n            req_text = self._find_section(full_text, ['–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è'])\n            if req_text:\n                requirements, advantages = self._parse_requirements_with_advantages(req_text)\n                vacancy.requirements = requirements\n                vacancy.advantages = advantages\n\n        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏\n        if responsibilities_text:\n            vacancy.responsibilities = self._parse_advanced_list(responsibilities_text)\n        else:\n            resp_text = self._find_section(full_text, ['–û–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏'])\n            if resp_text:\n                vacancy.responsibilities = self._parse_advanced_list(resp_text)\n\n        return vacancy\n\n    def _parse_requirements_with_advantages(self, text: str) -> Tuple[List[str], List[str]]:\n        \"\"\"–†–∞–∑–¥–µ–ª—è–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–Ω—ã–µ –∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞\"\"\"\n        requirements = []\n        advantages = []\n\n        # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏ –¥–æ –∏ –ø–æ—Å–ª–µ \"–ë—É–¥–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ–º\"\n        advantage_keywords = ['–±—É–¥–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ–º', '–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ–º –±—É–¥–µ—Ç', 'considered an advantage', '–±—É–¥–µ—Ç –ø–ª—é—Å–æ–º', '–ø–ª—é—Å–æ–º –±—É–¥–µ—Ç']\n\n        main_text = text\n        advantage_text = \"\"\n\n        for keyword in advantage_keywords:\n            if keyword in text.lower():\n                parts = re.split(keyword, text, flags=re.IGNORECASE)\n                if len(parts) >= 2:\n                    main_text = parts[0]\n                    advantage_text = parts[1]\n                    break\n\n        # –ü–∞—Ä—Å–∏–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è\n        requirements = self._parse_advanced_list(main_text)\n\n        # –ü–∞—Ä—Å–∏–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞\n        if advantage_text:\n            advantages = self._parse_advanced_list(advantage_text)\n\n        return requirements, advantages\n\n    def _parse_advanced_list(self, text: str) -> List[str]:\n        \"\"\"–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–æ–≤ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏\"\"\"\n        if not text:\n            return []\n\n        items = []\n        current_item = \"\"\n\n        # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–æ–∫–∏\n        lines = text.split('\\n')\n\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞—á–∞–ª–æ –Ω–æ–≤–æ–≥–æ –ø—É–Ω–∫—Ç–∞\n            is_new_item = any(line.startswith(prefix) for prefix in ['-', '‚Ä¢', '‚Äî', '*', '‚úì', '‚Üí']) or \\\n                         re.match(r'^\\d+[\\.\\)]', line) or \\\n                         re.match(r'^[a-z][\\)\\.]', line, re.IGNORECASE)\n\n            if is_new_item and current_item:\n                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –ø—É–Ω–∫—Ç\n                cleaned_item = self._clean_list_item(current_item)\n                if cleaned_item:\n                    items.append(cleaned_item)\n                current_item = line\n            else:\n                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ç–µ–∫—É—â–∏–π –ø—É–Ω–∫—Ç\n                if current_item:\n                    current_item += \" \" + line\n                else:\n                    current_item = line\n\n        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—É–Ω–∫—Ç\n        if current_item:\n            cleaned_item = self._clean_list_item(current_item)\n            if cleaned_item:\n                items.append(cleaned_item)\n\n        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: —Ä–∞–∑–¥–µ–ª—è–µ–º –ø—É–Ω–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ —Å–ª–∏—Ç—å—Å—è\n        final_items = []\n        for item in items:\n            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –ø—É–Ω–∫—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤\n            if ';' in item and len(item) > 50:  # –î–ª–∏–Ω–Ω—ã–π –ø—É–Ω–∫—Ç —Å —Ç–æ—á–∫–∞–º–∏ —Å –∑–∞–ø—è—Ç–æ–π\n                sub_items = item.split(';')\n                for sub_item in sub_items:\n                    cleaned_sub = sub_item.strip()\n                    if cleaned_sub and len(cleaned_sub) > 3:\n                        final_items.append(cleaned_sub)\n            else:\n                final_items.append(item)\n\n        return [item for item in final_items if item and len(item) > 3]\n\n    def _clean_list_item(self, item: str) -> str:\n        \"\"\"–û—á–∏—Å—Ç–∫–∞ –ø—É–Ω–∫—Ç–∞ —Å–ø–∏—Å–∫–∞ –æ—Ç –º–∞—Ä–∫–µ—Ä–æ–≤\"\"\"\n        # –£–±–∏—Ä–∞–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã —Å–ø–∏—Å–∫–∞\n        patterns = [\n            r'^[‚Ä¢\\-‚Äî*\\s]+',\n            r'^\\d+[\\.\\)]\\s*',\n            r'^[a-z][\\)\\.]\\s*',\n            r'^[‚úì‚Üí‚ñ∂]\\s*'\n        ]\n\n        for pattern in patterns:\n            item = re.sub(pattern, '', item, flags=re.IGNORECASE)\n\n        item = item.strip()\n\n        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –¥–µ—Ñ–∏—Å—ã –≤ –Ω–∞—á–∞–ª–µ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏\n        item = re.sub(r'^-\\s*', '', item)\n\n        return item\n\n    def _get_field_value(self, data: Dict[str, str], field_name: str) -> Optional[str]:\n        \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–ª—è\"\"\"\n        if field_name in data:\n            return data[field_name]\n\n        # –ü–æ–∏—Å–∫ –ø–æ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é\n        for key in data.keys():\n            if field_name.lower() in key.lower():\n                return data[key]\n\n        return None\n\n    def _find_section(self, text: str, section_names: List[str]) -> Optional[str]:\n        \"\"\"–ü–æ–∏—Å–∫ —Å–µ–∫—Ü–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é\"\"\"\n        for name in section_names:\n            pattern = rf'{name}.*?(?=\\n\\s*[–ê-–ØA-Z]|\\n\\n|$)'\n            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n            if match:\n                # –£–±–∏—Ä–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–µ–∫—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞\n                section_text = match.group(0)\n                return re.sub(rf'^{name}[\\s:\\-]*', '', section_text, flags=re.IGNORECASE).strip()\n        return None\n\n# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π DocumentProcessor\nclass DocumentProcessor:\n    def __init__(self):\n        self.vacancy_parser = AdvancedVacancyParser()\n        self.uploaded_files = {}\n\n    def upload_files(self):\n        print(\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã –≤–∞–∫–∞–Ω—Å–∏–π (DOCX –∏–ª–∏ RTF):\")\n        uploaded = files.upload()\n        self.uploaded_files = uploaded\n        return uploaded\n        \n    def process_vacancy(self, file_name: str) -> Vacancy:\n        if file_name in self.uploaded_files:\n            return self.vacancy_parser.parse_file(self.uploaded_files[file_name], file_name)\n        else:\n            raise ValueError(f\"–§–∞–π–ª {file_name} –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n\n# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞\nprocessor = DocumentProcessor()","metadata":{"id":"7ZHgKoQbuDNN","trusted":true,"execution":{"iopub.status.busy":"2025-09-06T08:56:26.965926Z","iopub.execute_input":"2025-09-06T08:56:26.966187Z","iopub.status.idle":"2025-09-06T08:56:27.043481Z","shell.execute_reply.started":"2025-09-06T08:56:26.966166Z","shell.execute_reply":"2025-09-06T08:56:27.042957Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"pip install docx2txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T08:54:36.171445Z","iopub.execute_input":"2025-09-06T08:54:36.171727Z","iopub.status.idle":"2025-09-06T08:54:39.389021Z","shell.execute_reply.started":"2025-09-06T08:54:36.171705Z","shell.execute_reply":"2025-09-06T08:54:39.388343Z"}},"outputs":[{"name":"stdout","text":"Collecting docx2txt\n  Downloading docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\nDownloading docx2txt-0.9-py3-none-any.whl (4.0 kB)\nInstalling collected packages: docx2txt\nSuccessfully installed docx2txt-0.9\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import re\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass\nimport docx\nfrom google.colab import files\nimport io\nimport os\nfrom docx2txt import process as rtf_process\n\n@dataclass\nclass Candidate:\n    experience_total: Optional[str] = None\n    work_experience: List[Dict[str, Any]] = None\n    education: List[Dict[str, Any]] = None\n    skills: List[str] = None\n    languages: List[Dict[str, Any]] = None\n    positions: List[str] = None\n\n    def __post_init__(self):\n        if self.work_experience is None:\n            self.work_experience = []\n        if self.education is None:\n            self.education = []\n        if self.skills is None:\n            self.skills = []\n        if self.languages is None:\n            self.languages = []\n        if self.positions is None:\n            self.positions = []\n\nclass ResumeParser:\n    \"\"\"–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Ä–µ–∑—é–º–µ\"\"\"\n\n    def parse_file(self, file_content: bytes, filename: str) -> Candidate:\n        \"\"\"–ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞ —Ä–µ–∑—é–º–µ\"\"\"\n        try:\n            if filename.lower().endswith('.docx'):\n                text = self._extract_text_from_docx(file_content)\n            elif filename.lower().endswith('.rtf'):\n                text = self._extract_text_from_rtf(file_content)\n            else:\n                raise ValueError(f\"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {filename}\")\n\n            return self.parse_text(text)\n\n        except Exception as e:\n            print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ä–µ–∑—é–º–µ: {e}\")\n            return Candidate()\n\n    def _extract_text_from_docx(self, file_content: bytes) -> str:\n        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX\"\"\"\n        doc = docx.Document(io.BytesIO(file_content))\n        text_parts = []\n\n        for para in doc.paragraphs:\n            if para.text.strip():\n                text_parts.append(para.text)\n\n        for table in doc.tables:\n            for row in table.rows:\n                row_text = []\n                for cell in row.cells:\n                    if cell.text.strip():\n                        row_text.append(cell.text.strip())\n                if row_text:\n                    text_parts.append(\" | \".join(row_text))\n\n        return \"\\n\".join(text_parts)\n\n    def _extract_text_from_rtf(self, file_content: bytes) -> str:\n        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ RTF —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π\"\"\"\n        try:\n            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –±–∞–π—Ç—ã –≤ —Å—Ç—Ä–æ–∫—É\n            rtf_text = file_content.decode('utf-8', errors='ignore')\n\n            # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ RTF\n            text = self._simple_rtf_to_text(rtf_text)\n            return text\n\n        except Exception as e:\n            print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ RTF: {e}\")\n            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥: –ø–æ–ø—Ä–æ–±—É–µ–º –ø—Ä–æ—Å—Ç–æ —É–¥–∞–ª–∏—Ç—å RTF —Ç–µ–≥–∏\n            return self._fallback_rtf_processing(file_content)\n\n    def _simple_rtf_to_text(self, rtf_text: str) -> str:\n        \"\"\"–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä RTF –≤ —Ç–µ–∫—Å—Ç\"\"\"\n        # –£–¥–∞–ª—è–µ–º RTF –∑–∞–≥–æ–ª–æ–≤–æ–∫\n        text = re.sub(r'\\\\[a-zA-Z]+\\d*', ' ', rtf_text)\n        text = re.sub(r'\\{.*?\\}', ' ', text)\n        text = re.sub(r'\\\\[{}]', '', text)\n\n        # –ó–∞–º–µ–Ω—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n        text = text.replace(r'\\par', '\\n')\n        text = text.replace(r'\\line', '\\n')\n        text = text.replace(r'\\tab', '\\t')\n        text = text.replace(r'\\emdash', '‚Äî')\n        text = text.replace(r'\\endash', '‚Äì')\n\n        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã\n        text = re.sub(r'\\s+', ' ', text)\n        text = re.sub(r'\\n\\s+', '\\n', text)\n\n        return text.strip()\n\n    def _fallback_rtf_processing(self, file_content: bytes) -> str:\n        \"\"\"–ê–≤–∞—Ä–∏–π–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ RTF\"\"\"\n        try:\n            # –ü—Ä–æ—Å—Ç–æ –ø—ã—Ç–∞–µ–º—Å—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ —Ç–µ–∫—Å—Ç, –∏–≥–Ω–æ—Ä–∏—Ä—É—è RTF —Ä–∞–∑–º–µ—Ç–∫—É\n            text = file_content.decode('utf-8', errors='ignore')\n            # –£–¥–∞–ª—è–µ–º —è–≤–Ω—ã–µ RTF —Ç–µ–≥–∏\n            text = re.sub(r'\\\\[a-zA-Z]+\\d*', ' ', text)\n            text = re.sub(r'[{}]', ' ', text)\n            text = re.sub(r'\\s+', ' ', text)\n            return text\n        except:\n            return \"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ RTF —Ñ–∞–π–ª–∞\"\n\n    def parse_text(self, text: str) -> Candidate:\n        \"\"\"–ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ —Ä–µ–∑—é–º–µ\"\"\"\n        candidate = Candidate()\n        text = self._normalize_text(text)\n\n        candidate.experience_total = self._extract_experience(text)\n        candidate.skills = self._extract_skills(text)\n        candidate.languages = self._extract_languages(text)\n        candidate.positions = self._extract_positions(text)\n\n        return candidate\n\n    def _normalize_text(self, text: str) -> str:\n        \"\"\"–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\"\"\"\n        text = re.sub(r'[‚Äì‚Äî‚àí‚Äê]', '-', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text\n\n    def _extract_experience(self, text: str) -> Optional[str]:\n        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø—ã—Ç–∞ —Ä–∞–±–æ—Ç—ã\"\"\"\n        patterns = [\n            r'–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã[\\s:\\-]*([^\\n]+?)(?=\\n|$)',\n            r'–°—Ç–∞–∂[\\s:\\-]*([^\\n]+?)(?=\\n|$)',\n            r'Experience[\\s:\\-]*([^\\n]+?)(?=\\n|$)',\n        ]\n\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                experience = match.group(1).strip()\n                experience = re.sub(r'[|\\-]\\s*', '', experience)\n                return experience\n        return None\n\n    def _extract_skills(self, text: str) -> List[str]:\n        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–≤—ã–∫–æ–≤\"\"\"\n        skills_section = self._find_section(text, ['–ù–∞–≤—ã–∫–∏', 'Skills'])\n        if not skills_section:\n            return []\n\n        skills_section = re.sub(r'^(–ù–∞–≤—ã–∫–∏|Skills)[\\s:\\-]*', '', skills_section, flags=re.IGNORECASE)\n        skills = re.findall(r'\\b[A-Z–ê-–Ø][A-Za-z–ê-–Ø–∞-—è0-9\\s\\/\\+\\.\\-]+\\b', skills_section)\n\n        filtered_skills = []\n        for skill in skills:\n            skill = skill.strip()\n            if (2 < len(skill) < 50 and\n                not any(x in skill.lower() for x in ['–Ω–∞–≤—ã–∫–∏', 'skills', '—è–∑—ã–∫–∏', 'languages']) and\n                not re.match(r'^[0-9\\s\\-]+$', skill)):\n                filtered_skills.append(skill)\n\n        return list(set(filtered_skills))\n\n    def _extract_languages(self, text: str) -> List[Dict[str, Any]]:\n        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤\"\"\"\n        languages_section = self._find_section(text, ['–Ø–∑—ã–∫–∏', 'Languages'])\n        if not languages_section:\n            return []\n\n        languages = []\n        patterns = [\n            r'([–ê-–Ø–∞-—èA-Za-z]+)[\\s\\-:]+([–ê-–Ø–∞-—èA-Za-z0-9\\s\\-]+)',\n            r'([–ê-–Ø–∞-—èA-Za-z]+)[\\s\\-]+—É—Ä–æ–≤–µ–Ω—å[\\s\\-]+([–ê-–Ø–∞-—èA-Za-z0-9\\s\\-]+)',\n        ]\n\n        for pattern in patterns:\n            matches = re.findall(pattern, languages_section, re.IGNORECASE)\n            for lang, level in matches:\n                lang = lang.strip()\n                level = level.strip()\n                if lang and level and len(lang) > 2:\n                    languages.append({'language': lang, 'level': level})\n\n        return languages\n\n    def _extract_positions(self, text: str) -> List[str]:\n        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π\"\"\"\n        positions = set()\n        position_patterns = [\n            r'(–í–µ–¥—É—â–∏–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç|–°—Ç–∞—Ä—à–∏–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç|–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç|–ò–Ω–∂–µ–Ω–µ—Ä|–†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫|–ê–Ω–∞–ª–∏—Ç–∏–∫|–ú–µ–Ω–µ–¥–∂–µ—Ä|–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä)',\n            r'(Senior|Junior|Lead|Principal)\\s+[A-Za-z–ê-–Ø–∞-—è]+',\n        ]\n\n        for pattern in position_patterns:\n            matches = re.findall(pattern, text, re.IGNORECASE)\n            for match in matches:\n                if isinstance(match, tuple):\n                    match = match[0]\n                positions.add(match.strip())\n\n        return list(positions)\n\n    def _find_section(self, text: str, section_names: List[str]) -> Optional[str]:\n        \"\"\"–ü–æ–∏—Å–∫ —Å–µ–∫—Ü–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é\"\"\"\n        for name in section_names:\n            pattern = rf'{name}.*?(?=\\n\\s*[–ê-–ØA-Z]|\\n\\n|$)'\n            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n            if match:\n                return match.group(0)\n        return None\n\n# –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä—Å–µ—Ä —Ä–µ–∑—é–º–µ –≤ DocumentProcessor\nclass DocumentProcessor:\n    def __init__(self):\n        self.resume_parser = ResumeParser()\n        self.uploaded_files = {}\n\n    def upload_files(self):\n        print(\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã (DOCX –∏–ª–∏ RTF):\")\n        uploaded = files.upload()\n        self.uploaded_files = uploaded\n        return uploaded\n\n    def upload_files(self, file_paths=None):\n        \"\"\"\n        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª—ã –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –ø—É—Ç—è–º.\n        :param file_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º (—Å—Ç—Ä–æ–∫–∏) –∏–ª–∏ None (–µ—Å–ª–∏ –∑–∞–≥—Ä—É–∂–∞—Ç—å –∏–∑ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏)\n        :return: –°–ª–æ–≤–∞—Ä—å —Å –∏–º–µ–Ω–∞–º–∏ —Ñ–∞–π–ª–æ–≤ –∏ –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º—ã–º\n        \"\"\"\n        if file_paths is None:\n            # –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã –ø—É—Ç–∏ ‚Äî –∏—â–µ–º –≤—Å–µ .docx –∏ .rtf –≤ —Ç–µ–∫—É—â–µ–π –ø–∞–ø–∫–µ\n            file_paths = []\n            for ext in ['docx', 'rtf']:\n                file_paths.extend([f for f in os.listdir('.') if f.lower().endswith(ext)])\n\n        uploaded = {}\n        for path in file_paths:\n            if not os.path.exists(path):\n                print(f\"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}\")\n                continue\n\n            try:\n                filename = os.path.basename(path)\n                print(f\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–∞–π–ª: {filename}\")\n\n                if path.lower().endswith('.docx'):\n                    doc = Document(path)\n                    text = ' '.join([para.text for para in doc.paragraphs])\n                elif path.lower().endswith('.rtf'):\n                    text = rtf_process(path)\n                else:\n                    print(f\"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {path}\")\n                    continue\n\n                uploaded[filename] = {\n                    'path': path,\n                    'text': text\n                }\n                print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω: {filename}\")\n\n            except Exception as e:\n                print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {path}: {e}\")\n\n        self.uploaded_files = uploaded\n        return uploaded\n\n    def process_resume(self, file_name: str) -> Candidate:\n        if file_name in self.uploaded_files:\n            return self.resume_parser.parse_file(self.uploaded_files[file_name], file_name)\n        else:\n            raise ValueError(f\"–§–∞–π–ª {file_name} –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n\n# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞\nprocessor = DocumentProcessor()","metadata":{"id":"T8iemUMWE7IQ","trusted":true,"execution":{"iopub.status.busy":"2025-09-06T09:00:49.465364Z","iopub.execute_input":"2025-09-06T09:00:49.465666Z","iopub.status.idle":"2025-09-06T09:00:49.491381Z","shell.execute_reply.started":"2025-09-06T09:00:49.465642Z","shell.execute_reply":"2025-09-06T09:00:49.490696Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª—ã\nprint(\"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∞—à–∏ —Ñ–∞–π–ª—ã (DOCX):\")\nuploaded = processor.upload_files(['/kaggle/input/111111111111111111/1  .rtf'])\nprint(\"–ó–∞–≥—Ä—É–∂–µ–Ω—ã —Ñ–∞–π–ª—ã:\", list(uploaded.keys()))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"UVBrSnwCtIA_","outputId":"21df6627-cec8-47a9-95fd-e6e7cf0b3f30","trusted":true,"execution":{"iopub.status.busy":"2025-09-06T09:07:26.144357Z","iopub.execute_input":"2025-09-06T09:07:26.145167Z","iopub.status.idle":"2025-09-06T09:07:26.160787Z","shell.execute_reply.started":"2025-09-06T09:07:26.145139Z","shell.execute_reply":"2025-09-06T09:07:26.160175Z"}},"outputs":[{"name":"stdout","text":"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∞—à–∏ —Ñ–∞–π–ª—ã (DOCX):\n–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–∞–π–ª: 1  .rtf\n–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ /kaggle/input/111111111111111111/1  .rtf: File is not a zip file\n–ó–∞–≥—Ä—É–∂–µ–Ω—ã —Ñ–∞–π–ª—ã: []\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –≤—ã–≤–æ–¥–æ–º\nprint(\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π...\")\nvacancy_files = [f for f in uploaded.keys() if any(kw in f.lower() for kw in ['–≤–∞–∫–∞–Ω—Å', '–æ–ø–∏—Å–∞–Ω', 'vacanc', 'job'])]\n\nfor vacancy_file in vacancy_files:\n    try:\n        print(f\"\\n{'='*80}\")\n        print(f\"üîç –û–ë–†–ê–ë–û–¢–ö–ê –í–ê–ö–ê–ù–°–ò–ò: {vacancy_file}\")\n        print(f\"{'='*80}\")\n\n        vacancy = processor.process_vacancy(vacancy_file)\n\n        print(\"‚úÖ –û–°–ù–û–í–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:\")\n        print(f\"   üìã –î–æ–ª–∂–Ω–æ—Å—Ç—å: {vacancy.title or '–ù–µ —É–∫–∞–∑–∞–Ω–∞'}\")\n        print(f\"   üèôÔ∏è –ì–æ—Ä–æ–¥: {vacancy.city or '–ù–µ —É–∫–∞–∑–∞–Ω'}\")\n        print(f\"   üíº –¢–∏–ø –∑–∞–Ω—è—Ç–æ—Å—Ç–∏: {vacancy.employment_type or '–ù–µ —É–∫–∞–∑–∞–Ω'}\")\n        print(f\"   üìÖ –ì—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã: {vacancy.work_schedule or '–ù–µ —É–∫–∞–∑–∞–Ω'}\")\n        print(f\"   üéì –£—Ä–æ–≤–µ–Ω—å –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: {vacancy.education_level or '–ù–µ —É–∫–∞–∑–∞–Ω'}\")\n        print(f\"   ‚è≥ –¢—Ä–µ–±—É–µ–º—ã–π –æ–ø—ã—Ç: {vacancy.experience_required or '–ù–µ —É–∫–∞–∑–∞–Ω'}\")\n\n        print(f\"\\nüìã –û–°–ù–û–í–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø ({len(vacancy.requirements)}):\")\n        for i, req in enumerate(vacancy.requirements, 1):\n            print(f\"   {i:2d}. {req}\")\n\n        if vacancy.advantages:\n            print(f\"\\n‚≠ê –ë–£–î–ï–¢ –ü–†–ï–ò–ú–£–©–ï–°–¢–í–û–ú ({len(vacancy.advantages)}):\")\n            for i, advantage in enumerate(vacancy.advantages, 1):\n                print(f\"   {i:2d}. {advantage}\")\n\n        print(f\"\\nüìù –û–ë–Ø–ó–ê–ù–ù–û–°–¢–ò ({len(vacancy.responsibilities)}):\")\n        for i, resp in enumerate(vacancy.responsibilities, 1):\n            print(f\"   {i:2d}. {resp}\")\n\n    except Exception as e:\n        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {vacancy_file}: {e}\")\n        import traceback\n        traceback.print_exc()","metadata":{"id":"utZfBtjHtItP"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—é–º–µ\nprint(\"\\n–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—é–º–µ...\")\nresume_files = [f for f in uploaded.keys() if any(kw in f.lower() for kw in ['—Ä–µ–∑—é–º–µ', 'cv', 'resume', '–æ–±—Ä–∞–∑–µ—Ü'])]\n\nfor resume_file in resume_files:\n    try:\n        print(f\"\\nüîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ä–µ–∑—é–º–µ: {resume_file}\")\n        candidate = processor.process_resume(resume_file)\n\n        print(\"‚úÖ –î–ê–ù–ù–´–ï –ö–ê–ù–î–ò–î–ê–¢–ê:\")\n        print(f\"üìÖ –û–±—â–∏–π –æ–ø—ã—Ç: {candidate.experience_total or '–ù–µ —É–∫–∞–∑–∞–Ω'}\")\n\n        print(f\"\\nüíº –î–æ–ª–∂–Ω–æ—Å—Ç–∏ ({len(candidate.positions)}):\")\n        for i, pos in enumerate(candidate.positions[:5], 1):\n            print(f\"   {i}. {pos}\")\n\n        print(f\"\\nüõ†Ô∏è  –ù–∞–≤—ã–∫–∏ ({len(candidate.skills)}):\")\n        for i, skill in enumerate(candidate.skills[:15], 1):\n            print(f\"   {i}. {skill}\")\n\n        print(f\"\\nüåê –Ø–∑—ã–∫–∏ ({len(candidate.languages)}):\")\n        for i, lang in enumerate(candidate.languages, 1):\n            print(f\"   {i}. {lang['language']} - {lang['level']}\")\n\n    except Exception as e:\n        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {resume_file}: {e}\")","metadata":{"id":"USwA3gn8ufPH"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"WTVC1mukr-eO"},"outputs":[],"execution_count":null}]}